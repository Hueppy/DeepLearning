{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58615c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14a0a2",
   "metadata": {},
   "source": [
    "### Prepare Datasets\n",
    "\n",
    " * Load train dataset and split into train and validation sets\n",
    " * Load test dataset\n",
    " * Augment train data with random flip, rotation, translation and zoom operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26a4815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22573 files belonging to 25 classes.\n",
      "Using 20316 files for training.\n",
      "Using 2257 files for validation.\n",
      "Found 2500 files belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train, validation = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../Dataset/train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(224,224),\n",
    "    seed=815,\n",
    "    validation_split=0.1,\n",
    "    subset='both'\n",
    ")\n",
    "\n",
    "test = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../Dataset/test',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(224,224)\n",
    ")\n",
    "\n",
    "def prepare(dataset):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.5),\n",
    "        tf.keras.layers.RandomTranslation(0.33, 0.33),\n",
    "        tf.keras.layers.RandomZoom(0.33),\n",
    "    ])\n",
    "\n",
    "    augmented = dataset\n",
    "    for _ in range(9):\n",
    "        augmented = augmented.concatenate(\n",
    "            dataset.map(\n",
    "                lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return augmented.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train = prepare(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c547e0a",
   "metadata": {},
   "source": [
    "## Distiller class\n",
    "https://keras.io/examples/vision/knowledge_distillation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0a4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/vision/knowledge_distillation\n",
    "class Distiller(tf.keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bf5f7",
   "metadata": {},
   "source": [
    "### Create model\n",
    " * Convolutional block\n",
    "   * Residual \"skipping\" layer\n",
    "   * Batch normalization around each convolutional layer\n",
    "   * (2,2) Max Pooling with stride (2,2) after all convulutional layers\n",
    " * Flatten\n",
    " * Dense layers with ReLu activation\n",
    " * Output layer with Softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7976126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(conv_layers, dense_layers):    \n",
    "    img_input = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "    x = tf.keras.layers.BatchNormalization()(img_input)\n",
    "\n",
    "    for filters, kernel_size, depth, pool in conv_layers:\n",
    "        shortcut = tf.keras.layers.Conv2D(\n",
    "            filters, \n",
    "            1, \n",
    "            activation='relu')(x)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "        \n",
    "        for i in range(depth):\n",
    "            x = tf.keras.layers.Conv2D(                \n",
    "                filters,\n",
    "                kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same'\n",
    "            )(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "                \n",
    "        x = tf.keras.layers.Add()([shortcut,x])\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "            \n",
    "        if pool > 1:\n",
    "            x = tf.keras.layers.MaxPool2D(\n",
    "                pool_size=pool\n",
    "            )(x)\n",
    "            \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    for units in dense_layers:\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu'\n",
    "        )(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(\n",
    "        25,\n",
    "        activation='softmax'\n",
    "    )(x)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(img_input, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2ef05",
   "metadata": {},
   "source": [
    "### Run \n",
    " * Load teacher model and weights using supplied id\n",
    " * Instantiate model with set parameters\n",
    " * Save model json to file\n",
    " * Initialize learning rate scheduler\n",
    " * Instantiate and compile distiller class\n",
    " * Fit student model using distiller class\n",
    "   * Early stopping callback\n",
    "   * TensorBoard callback\n",
    " * Fit model to training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297aa8ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(i, id, teacher_id):    \n",
    "    json = ''\n",
    "    with open(f'models/{teacher_id}.json', 'r') as json_file:\n",
    "        json = json_file.read()\n",
    "    \n",
    "    teacher = tf.keras.models.model_from_json(json)\n",
    "    teacher.load_weights(f'checkpoints/{teacher_id}')\n",
    "    #teacher.summary()\n",
    "    \n",
    "    student = generate_model(\n",
    "        [\n",
    "            # filters, kernel_size, depth, pool\n",
    "            (16, 3, 2, 2),\n",
    "            (32, 3, 2, 2),\n",
    "            (64, 3, 2, 2),\n",
    "        ], \n",
    "        [\n",
    "            # units\n",
    "            96,\n",
    "            128,\n",
    "        ])\n",
    "    \n",
    "    with open(f'models/{id}-{i}.json', 'w') as f:\n",
    "        f.write(student.to_json())\n",
    "        \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.002, \n",
    "        decay_steps=6350, \n",
    "        decay_rate=0.67\n",
    "    )\n",
    "    \n",
    "    distiller = Distiller(student=student, teacher=teacher)\n",
    "    distiller.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        student_loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy'],\n",
    "        distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "        alpha=0.1,\n",
    "        temperature=5,\n",
    "    )\n",
    "        \n",
    "    distiller.fit(\n",
    "        train,\n",
    "        validation_data=validation,\n",
    "        epochs=10, \n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_student_loss'),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=f'logs/fit/{id}-{i}'\n",
    "            ),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(\n",
    "            #    filepath=f'checkpoints/{id}',\n",
    "            #    monitor='val_accuracy',\n",
    "            #)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    student.save_weights(f'checkpoints/{id}-{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4746ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3906/6350 [=================>............] - ETA: 6:05 - accuracy: 0.2822 - student_loss: 2.3851 - distillation_loss: 0.0175"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "for i in range(1):\n",
    "    run(i, f'{now}', '20230530-223331')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75087ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
